#use pip install numpy opencv-python pyaudio

import numpy as np
import cv2
import pyaudio

# --- Autoencoder with Novelty-Based Backpropagation ---
class AutoEncoder:
    def __init__(self, input_dim, hidden_dim, learning_rate=1e-4):
        # Encoder weights (input -> hidden)
        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.1
        self.b1 = np.random.randn(hidden_dim) * 0.1
        # Decoder weights (hidden -> output)
        self.W2 = np.random.randn(hidden_dim, input_dim) * 0.1
        self.b2 = np.random.randn(input_dim) * 0.1
        self.learning_rate = learning_rate

    def forward(self, x):
        # Save input for backprop
        self.x = x
        # Encoder: linear then ReLU activation
        self.h_linear = np.dot(x, self.W1) + self.b1
        self.h = np.maximum(0, self.h_linear)  # ReLU activation
        # Decoder: linear then Sigmoid activation to yield output in [0,1]
        self.out_linear = np.dot(self.h, self.W2) + self.b2
        self.out = 1.0 / (1.0 + np.exp(-self.out_linear))  # Sigmoid activation
        return self.out

    def backward(self, output, target):
        # Compute loss derivative; using Mean Squared Error (MSE)
        # Loss: L = 0.5 * ||target - output||^2, so dL/dout = output - target
        d_out = output - target  # shape: (input_dim,)
        # Derivative for the sigmoid activation
        d_out_linear = d_out * output * (1 - output)

        # Gradients for decoder parameters
        dW2 = np.outer(self.h, d_out_linear)
        db2 = d_out_linear

        # Backprop into the hidden layer
        d_hidden = np.dot(d_out_linear, self.W2.T)
        d_h_linear = d_hidden * (self.h_linear > 0)  # derivative of ReLU

        # Gradients for encoder parameters
        dW1 = np.outer(self.x, d_h_linear)
        db1 = d_h_linear

        # Update the parameters using gradient descent
        self.W2 -= self.learning_rate * dW2
        self.b2 -= self.learning_rate * db2
        self.W1 -= self.learning_rate * dW1
        self.b1 -= self.learning_rate * db1

    def train_step(self, x, target):
        output = self.forward(x)
        self.backward(output, target)
        loss = np.mean(0.5 * (target - output) ** 2)
        return output, loss

# --- Audio Capture Class ---
class AudioCapture:
    def __init__(self, rate=44100, chunk=1024):
        self.rate = rate
        self.chunk = chunk
        self.pa = pyaudio.PyAudio()
        self.stream = self.pa.open(format=pyaudio.paInt16,
                                   channels=1,
                                   rate=self.rate,
                                   input=True,
                                   frames_per_buffer=self.chunk)

    def get_audio(self):
        data = self.stream.read(self.chunk, exception_on_overflow=False)
        return np.frombuffer(data, dtype=np.int16)

    def close(self):
        self.stream.stop_stream()
        self.stream.close()
        self.pa.terminate()

# --- Main Loop ---
def main():
    # Video capture setup
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("Error: Could not open video stream.")
        return

    # Audio capture setup
    audio_capture = AudioCapture(rate=44100, chunk=1024)
    
    img_size = 128        # We resize the captured frame to 128x128 pixels
    audio_size = 1024     # Number of audio samples per chunk
    flat_img_size = img_size * img_size
    input_dim = flat_img_size + audio_size  # Total input dimension

    # Define the autoencoder
    # hidden_dim can be chosen arbitrarily; for demonstration, we choose a moderate size.
    hidden_dim = 1024  
    autoencoder = AutoEncoder(input_dim, hidden_dim, learning_rate=1e-4)
    
    # Optionally, you could set a base learning rate and scale it with the novelty
    base_learning_rate = 1e-4
    novelty_scaling = 10.0  # how strongly the novelty error will scale the update

    while True:
        ret, frame = cap.read()
        if not ret:
            break

        # --- Image Processing ---
        # Resize and convert to grayscale; normalize to range [0,1]
        frame_resized = cv2.resize(frame, (img_size, img_size))
        gray_frame = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)
        flat_image = gray_frame.flatten().astype(np.float32) / 255.0

        # --- Audio Processing ---
        audio_chunk = audio_capture.get_audio()
        # Normalize audio (16-bit int) to roughly the range [0,1]
        audio_data = np.array(audio_chunk, dtype=np.float32) / 32768.0

        # --- Form Combined Input ---
        # The input vector is a concatenation of the flat image and the audio data.
        input_vector = np.concatenate([flat_image, audio_data])

        # --- Autoencoder Forward/Backward Pass ---
        # Here the target is the input itself, so the network tries to reconstruct it.
        output_vector, loss = autoencoder.train_step(input_vector, input_vector)

        # Compute a novelty measure using the reconstruction error (mean absolute error)
        novelty = np.mean(np.abs(input_vector - output_vector))

        # Adjust the learning rate dynamically based on novelty
        # When novelty is high, we might want to take larger steps.
        autoencoder.learning_rate = base_learning_rate * (1 + novelty_scaling * novelty)

        # --- Visualization ---
        # Extract the reconstructed image portion (first flat_img_size elements)
        reconstructed_image = output_vector[:flat_img_size]
        reconstructed_image = (reconstructed_image.reshape(img_size, img_size) * 255).astype(np.uint8)

        # Also scale the original grayscale image for display
        original_display = (flat_image.reshape(img_size, img_size) * 255).astype(np.uint8)

        # Overlay text information on the displayed images (e.g., loss, novelty)
        cv2.putText(original_display, f'Loss: {loss:.4f}', (5, 15),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,), 1)
        cv2.putText(original_display, f'Novelty: {novelty:.4f}', (5, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,), 1)
        cv2.putText(reconstructed_image, f'Loss: {loss:.4f}', (5, 15),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,), 1)
        cv2.putText(reconstructed_image, f'Novelty: {novelty:.4f}', (5, 30),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,), 1)

        # Show both the original and reconstructed images
        cv2.imshow("Original Frame", original_display)
        cv2.imshow("Reconstructed Frame", reconstructed_image)

        # Break the loop when the user presses 'q'
        if cv2.waitKey(1) & 0xFF == ord('q'):
            break

    cap.release()
    audio_capture.close()
    cv2.destroyAllWindows()

if __name__ == '__main__':
    main()
