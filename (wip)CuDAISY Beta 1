"""
LDM_Project_POC.py

Lightweight prototype for transforming webcam frames into a compact pixel representation
and reconstructing them in real time. Intended as a proof-of-concept for a small research
project â€” optimized for clarity and ease of use, and packaged for GitHub.

Features
- Real-time webcam / OBS virtual camera input
- PyTorch implementation with optional CUDA acceleration
- Display: original frame | pixel-vector visualization | reconstructed image
- Online and lightweight per-frame adaptation modes

Requirements
- Python 3.8+
- numpy, opencv-python, matplotlib (optional), torch

Install
```
pip install numpy opencv-python matplotlib torch
```

Quick start
- Realtime (default device):
  `python LDM_Project_POC.py --realtime`
- Realtime (specific device):
  `python LDM_Project_POC.py --realtime --device 1`
- Realtime with CUDA (if available):
  `python LDM_Project_POC.py --realtime --device 1 --cuda`
- Run synthetic demo:
  `python LDM_Project_POC.py --demo`

License: MIT
"""

import argparse
import os
import math
import time
import pickle

import numpy as np
import cv2
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
import torch.optim as optim


# -------- Helpers --------

def choose_device(use_cuda: bool):
    return torch.device('cuda') if (use_cuda and torch.cuda.is_available()) else torch.device('cpu')


# -------- Pixel mapper --------
class PixelVector:
    def __init__(self, vector_size, img_shape=None):
        self.vector_size = vector_size
        if img_shape is None:
            for pref in (128, 64, 32):
                if vector_size % pref == 0:
                    w = pref
                    h = vector_size // w
                    img_shape = (h, w)
                    break
            if img_shape is None:
                s = int(math.sqrt(vector_size))
                h = s
                w = (vector_size + s - 1) // s
                img_shape = (h, w)
        self.img_shape = img_shape
        assert self.img_shape[0] * self.img_shape[1] == vector_size, "img shape must match vector size"

    def to_image(self, vec):
        return vec.reshape(self.img_shape)


# -------- Memory (simple store) --------
class MemoryBank:
    def __init__(self, capacity=8192, prototype_dim=8192, threshold=0.72, device=torch.device('cpu')):
        self.capacity = capacity
        self.prototype_dim = prototype_dim
        self.threshold = threshold
        self.device = device
        self.prototypes = torch.empty((0, prototype_dim), device=self.device)

    def is_new(self, vec: torch.Tensor):
        if self.prototypes.shape[0] == 0:
            return True, None
        v = vec / (vec.norm() + 1e-8)
        P = self.prototypes / (self.prototypes.norm(dim=1, keepdim=True) + 1e-8)
        sims = P.matmul(v)
        best, idx = torch.max(sims, dim=0)
        return (best.item() < self.threshold), int(idx.item())

    def store(self, vec: torch.Tensor):
        vec = vec.reshape(1, -1).to(self.device)
        if self.prototypes.shape[0] < self.capacity:
            self.prototypes = torch.cat([self.prototypes, vec], dim=0)
        else:
            i = torch.randint(0, self.capacity, (1,)).item()
            self.prototypes[i] = vec


# -------- Model --------
class LDMModel(nn.Module):
    def __init__(self, input_dim, enc_hidden, pixel_size, inf_hidden):
        super().__init__()
        self.encoder = nn.Sequential(nn.Linear(input_dim, enc_hidden), nn.LeakyReLU(0.1))
        self.to_pixel = nn.Sequential(nn.Linear(enc_hidden, pixel_size), nn.Sigmoid())
        self.infer1 = nn.Sequential(nn.Linear(pixel_size, inf_hidden), nn.LeakyReLU(0.1))
        self.infer2 = nn.Sequential(nn.Linear(inf_hidden, input_dim), nn.Sigmoid())
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_uniform_(m.weight, a=0.1)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def encode(self, x):
        h = self.encoder(x)
        p = self.to_pixel(h)
        return h, p

    def infer(self, p):
        h = self.infer1(p)
        out = self.infer2(h)
        return out

    def forward(self, x):
        h, p = self.encode(x)
        out = self.infer(p)
        return out, p


# -------- System wrapper --------
class LDMSystem:
    def __init__(self, scale=16, base_input=64, base_enc=128, base_pixel=256, base_inf=128, lr=5e-4, device=torch.device('cpu')):
        self.scale = scale
        self.input_dim = base_input * scale
        self.enc_hidden = base_enc * scale
        self.pixel_size = base_pixel * scale
        self.inf_hidden = base_inf * scale
        self.device = device

        self.model = LDMModel(self.input_dim, self.enc_hidden, self.pixel_size, self.inf_hidden).to(self.device)
        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)

        self.memory = MemoryBank(capacity=8192, prototype_dim=self.pixel_size, threshold=0.72)
        self.pixel_mapper = PixelVector(self.pixel_size)

        if self.input_dim % 128 == 0:
            self.src_hw = (self.input_dim // 128, 128)
        elif self.input_dim % 64 == 0:
            self.src_hw = (self.input_dim // 64, 64)
        else:
            s = int(math.sqrt(self.input_dim))
            self.src_hw = (s, (self.input_dim + s - 1) // s)

        self.loss_fn = nn.MSELoss()
        self.running_mean = 0.0
        self.running_var = 1.0
        self.running_count = 0

    def _update_stats(self, x_np):
        x = x_np.reshape(-1)
        self.running_count += 1
        if self.running_count == 1:
            self.running_mean = x
            self.running_var = np.zeros_like(x)
        else:
            old_mean = self.running_mean
            new_mean = old_mean + (x - old_mean) / self.running_count
            self.running_var = ((self.running_count - 2) / (self.running_count - 1)) * self.running_var + (x - old_mean) * (x - new_mean)
            self.running_mean = new_mean

    def _normalize(self, x_np):
        if self.running_count < 2:
            return x_np
        std = np.sqrt(self.running_var / (self.running_count - 1) + 1e-6)
        normed = (x_np.reshape(-1) - self.running_mean) / std
        normed = (normed - normed.min()) / (normed.max() - normed.min() + 1e-9)
        return normed.reshape(-1, 1)

    def train_step(self, x_np):
        self._update_stats(x_np)
        x_norm = self._normalize(x_np)
        x = torch.tensor(x_norm.T, dtype=torch.float32, device=self.device)
        self.optimizer.zero_grad()
        out, p = self.model(x)
        loss = self.loss_fn(out, x)
        loss.backward()
        self.optimizer.step()

        p_flat = p.detach().cpu().reshape(-1)
        is_new, idx = self.memory.is_new(p_flat)
        if is_new:
            self.memory.store(p_flat)

        return loss.item(), is_new

    def adapt_step(self, x_np, adapt_lr=1e-4):
        x_norm = self._normalize(x_np)
        x = torch.tensor(x_norm.T, dtype=torch.float32, device=self.device)
        enc_params = list(self.model.encoder.parameters()) + list(self.model.to_pixel.parameters())
        adapt_opt = optim.Adam(enc_params, lr=adapt_lr)
        adapt_opt.zero_grad()
        out, p = self.model(x)
        loss = self.loss_fn(out, x)
        loss.backward()
        adapt_opt.step()
        return loss.item()

    def infer_only(self, x_np):
        x_norm = self._normalize(x_np)
        x = torch.tensor(x_norm.T, dtype=torch.float32, device=self.device)
        with torch.no_grad():
            out, p = self.model(x)
        return out.detach().cpu().numpy().reshape(-1), p.detach().cpu().numpy().reshape(-1)

    def pixel_to_display(self, p_vec, upsample=512):
        vec = p_vec.reshape(-1)
        L = vec.size
        if L % 3 == 0:
            c = L // 3
            r = vec[0:c].reshape(self.pixel_mapper.img_shape)
            g = vec[c:2*c].reshape(self.pixel_mapper.img_shape)
            b = vec[2*c:3*c].reshape(self.pixel_mapper.img_shape)
            rgb = np.stack([r, g, b], axis=2)
            img = np.clip(rgb, 0, 1)
            img_u = cv2.resize((img * 255).astype(np.uint8), (upsample, upsample), interpolation=cv2.INTER_NEAREST)
            return cv2.cvtColor(img_u, cv2.COLOR_RGB2BGR)
        else:
            img = vec.reshape(self.pixel_mapper.img_shape)
            img = np.clip(img, 0, 1)
            img_u = cv2.resize((img * 255).astype(np.uint8), (upsample, upsample), interpolation=cv2.INTER_NEAREST)
            return cv2.cvtColor(img_u, cv2.COLOR_GRAY2BGR)

    def recon_to_display(self, recon_vec, upsample=512):
        img = recon_vec.reshape(self.src_hw)
        img = np.clip(img, 0, 1)
        img_u = cv2.resize((img * 255).astype(np.uint8), (upsample, upsample), interpolation=cv2.INTER_LINEAR)
        return cv2.cvtColor(img_u, cv2.COLOR_GRAY2BGR)

    def save_checkpoint(self, path="ldm_pytorch_checkpoint.pt"):
        state = {'model': self.model.state_dict(), 'optimizer': self.optimizer.state_dict(), 'memory': self.memory.prototypes.numpy(), 'running_mean': self.running_mean, 'running_var': self.running_var, 'running_count': self.running_count}
        torch.save(state, path)

    def load_checkpoint(self, path="ldm_pytorch_checkpoint.pt"):
        if not os.path.exists(path):
            return False
        state = torch.load(path, map_location=self.device)
        self.model.load_state_dict(state['model'])
        try:
            self.optimizer.load_state_dict(state['optimizer'])
        except Exception:
            pass
        self.memory.prototypes = torch.tensor(state.get('memory', np.zeros((0, self.memory.prototype_dim))), dtype=torch.float32)
        self.running_mean = state.get('running_mean', self.running_mean)
        self.running_var = state.get('running_var', self.running_var)
        self.running_count = state.get('running_count', self.running_count)
        return True


# -------- Frame -> vector --------

def frame_to_input_vector(frame_gray, input_dim, src_hw):
    small = cv2.resize(frame_gray, (src_hw[1], src_hw[0]), interpolation=cv2.INTER_AREA)
    vec = (small.astype(np.float32) / 255.0).reshape(-1)
    if vec.shape[0] != input_dim:
        if vec.shape[0] > input_dim:
            vec = vec[:input_dim]
        else:
            pad = np.zeros(input_dim - vec.shape[0], dtype=vec.dtype)
            vec = np.concatenate([vec, pad])
    return vec.reshape(-1, 1)


# -------- Demo --------

def synthetic_input_patterns(n_patterns=32, input_dim=4096, noise=0.04):
    bases = []
    rng = np.random.RandomState(0)
    for i in range(n_patterns):
        v = np.zeros(input_dim)
        idx = rng.choice(range(input_dim), size=max(8, input_dim // 16), replace=False)
        v[idx] = 1.0
        bases.append(v)
    i = 0
    while True:
        p = bases[i % len(bases)] + rng.randn(input_dim) * noise
        p = np.clip(p, 0, 1)
        yield p.reshape(-1, 1)
        i += 1


def run_demo(steps=400, show_every=100, scale=16, use_cuda=False):
    device = choose_device(use_cuda)
    sys = LDMSystem(scale=scale, device=device)
    gen = synthetic_input_patterns(n_patterns=40, input_dim=sys.input_dim, noise=0.05)

    losses = []
    nov_counts = 0
    for t in range(steps):
        x = next(gen)
        loss, novel = sys.train_step(x)
        losses.append(loss)
        if novel:
            nov_counts += 1
        if (t+1) % show_every == 0:
            print(f"step {t+1}/{steps}  loss={np.mean(losses[-show_every:]):.5f}  novel_count={nov_counts}")
            out_np, p_np = sys.infer_only(x)
            plt.figure(figsize=(6,6))
            plt.imshow(p_np.reshape(sys.pixel_mapper.img_shape), cmap='gray', vmin=0, vmax=1)
            plt.title(f"step {t+1} pixel (novel={novel})")
            plt.axis('off')
            plt.show()

    plt.figure()
    plt.plot(losses)
    plt.title('Training loss')
    plt.xlabel('step')
    plt.ylabel('mse')
    plt.show()
    sys.save_checkpoint()
    print('Demo complete. Checkpoint saved to ldm_pytorch_checkpoint.pt')


# -------- Realtime --------

def run_realtime(device=0, train=False, scale=16, use_cuda=False, cam_backend=None, adapt=True):
    device_t = choose_device(use_cuda)

    cap = None
    try:
        if cam_backend == 'dshow' and hasattr(cv2, 'CAP_DSHOW'):
            cap = cv2.VideoCapture(device, cv2.CAP_DSHOW)
        else:
            cap = cv2.VideoCapture(device)
            if not cap.isOpened() and hasattr(cv2, 'CAP_DSHOW'):
                cap.release()
                cap = cv2.VideoCapture(device, cv2.CAP_DSHOW)
    except Exception:
        cap = cv2.VideoCapture(device)

    if not cap.isOpened():
        raise RuntimeError(f"Could not open camera device {device}. Try a different index or start OBS Virtual Camera.")

    sys = LDMSystem(scale=scale, device=device_t)

    print(f"Realtime mode started. device={device} use_cuda={use_cuda} device_t={device_t}. Press 'q' to quit. 's' to save.")

    try:
        last_frame_time = time.time()
        while True:
            ret, frame = cap.read()
            if not ret:
                print('Frame read failed; exiting')
                break
            frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            inp = frame_to_input_vector(frame_gray, input_dim=sys.input_dim, src_hw=sys.src_hw)

            if train:
                loss, novel = sys.train_step(inp)
            else:
                if adapt:
                    loss = sys.adapt_step(inp, adapt_lr=2e-4)
                    novel = False
                else:
                    out_np, p_np = sys.infer_only(inp)
                    loss, novel = None, False

            out_np, p_np = sys.infer_only(inp)

            orig_display = cv2.resize(frame, (480, 360))
            pixel_display = sys.pixel_to_display(p_np, upsample=360)
            recon_display = sys.recon_to_display(out_np, upsample=360)

            combined = np.hstack([orig_display, pixel_display, recon_display])
            cv2.imshow('LDM - original | pixel | reconstruction', combined)

            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            if key == ord('s'):
                sys.save_checkpoint()
                print('Checkpoint saved')
    finally:
        cap.release()
        cv2.destroyAllWindows()


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--demo', action='store_true', help='Run synthetic demo')
    parser.add_argument('--realtime', action='store_true', help='Run realtime camera mode')
    parser.add_argument('--device', type=int, default=0, help='Camera device index for OpenCV (OBS virtual camera likely not 0)')
    parser.add_argument('--train', action='store_true', help='Enable online training in realtime mode')
    parser.add_argument('--cuda', action='store_true', help='Use CUDA if available')
    parser.add_argument('--scale', type=int, default=16, help='Scale factor to multiply base sizes (default 16)')
    parser.add_argument('--backend', choices=['dshow', 'default'], default='default', help='Camera backend for OpenCV on Windows')
    parser.add_argument('--adapt', action='store_true', help='Enable tiny per-frame adaptation to avoid stagnation (default on)')
    args = parser.parse_args()

    try:
        if args.demo:
            run_demo(steps=400, show_every=100, scale=args.scale, use_cuda=args.cuda)
        else:
            if not args.realtime:
                print('No mode selected. Defaulting to realtime mode. Use --demo for offline demo.')
            adapt_flag = args.adapt or True
            run_realtime(device=args.device, train=args.train, scale=args.scale, use_cuda=args.cuda, cam_backend=args.backend, adapt=adapt_flag)
    except RuntimeError as e:
        print('Realtime startup error:', e)
        print('Make sure OBS Virtual Camera is running or device index is correct.')
        try:
            input('Press Enter to exit...')
        except Exception:
            pass
    except Exception as e:
        print('Unhandled exception:', repr(e))
        try:
            input('Press Enter to exit...')
        except Exception:
            pass
