#!/usr/bin/env python3
import argparse
import math
import os
import time
import csv
import tempfile
from collections import deque
from typing import Tuple

import numpy as np

# Optional plotting
try:
    import matplotlib
    matplotlib.use("Agg")
    import matplotlib.pyplot as plt
    _HAS_MPL = True
except Exception:
    _HAS_MPL = False

# Optional GUI (Tkinter) will be imported later only if requested and available.

# -----------------------------
# CONFIGURATION (easy to change)
# -----------------------------
# Base dimensions (change these to change every benchmark's baseline)
BASE_PIXEL_DIM = 8                       # base pixel width/height (pixel_shape = (BASE_PIXEL_DIM, BASE_PIXEL_DIM))
BASE_HIDDEN_SMALL = (64, 32)             # used for small tasks
BASE_HIDDEN_MED   = (128, 64)            # medium
BASE_HIDDEN_LARGE = (256, 128)           # large

# Default global scaling factor (change this to grow/shrink all networks)
# Setting SCALE_FACTOR = 2 doubles the sizes (hidden and latent/pixels) as you requested.
SCALE_FACTOR = 2.0

# Training defaults
DEFAULT_EPOCHS = 30
DEFAULT_LR = 1e-3

# File name for the copy placed into Downloads
DOWNLOADS_COPY_NAME = "ldm_summary.txt"

# -----------------------------
# Minimal neural utils (NumPy)
# -----------------------------
def glorot_init(in_dim, out_dim):
    limit = math.sqrt(6.0 / (in_dim + out_dim))
    return np.random.uniform(-limit, limit, size=(in_dim, out_dim)).astype(np.float32)

def relu(x):
    return np.maximum(0, x)

def relu_deriv(x):
    return (x > 0).astype(np.float32)

def sigmoid(x):
    return 1.0 / (1.0 + np.exp(-x))

def tanh(x):
    return np.tanh(x)

def mse(y_pred, y_true):
    return np.mean((y_pred - y_true) ** 2)

def mse_grad(y_pred, y_true):
    # careful with shapes
    return (2.0 * (y_pred - y_true)) / y_pred.size


# -----------------------------
# Liquid Data Matrix (stateful latent)
# -----------------------------
class LiquidDataMatrix:
    """
    Stateful LDM: keeps a persistent latent vector self.S that is updated between batches.
    The inference network receives the concatenation [pixel, S] as input.
    The write-gate uses novelty to decide how much to write into S.
    """
    def __init__(self, input_size, pixel_shape=(8,8), hidden_sizes=(128, 64), output_size=1,
                 lr=1e-3, novelty_mem=256, novelty_boost=3.0, decay=0.0, seed=None):
        if seed is not None:
            np.random.seed(seed)
        self.input_size = input_size
        self.pixel_shape = pixel_shape
        self.pixel_size = pixel_shape[0] * pixel_shape[1]
        self.hidden_sizes = hidden_sizes
        self.output_size = output_size
        self.lr = lr
        self.decay = decay

        # Encoder (input -> pixel vector)
        self.W_enc = glorot_init(input_size, self.pixel_size)
        self.b_enc = np.zeros((self.pixel_size,), dtype=np.float32)

        # Persistent latent state (vector)
        self.S = np.zeros((self.pixel_size,), dtype=np.float32)

        # Write network: (pixel || S) -> candidate, gate
        self.W_write = glorot_init(self.pixel_size * 2, self.pixel_size)
        self.b_write = np.zeros((self.pixel_size,), dtype=np.float32)
        self.W_gate = glorot_init(self.pixel_size * 2, self.pixel_size)
        self.b_gate = np.zeros((self.pixel_size,), dtype=np.float32)

        # Inference perceptron now takes [pixel || S] as input
        sizes = [self.pixel_size * 2] + list(hidden_sizes)
        self.Ws = [glorot_init(sizes[i], sizes[i+1]) for i in range(len(sizes)-1)]
        self.bs = [np.zeros((s.shape[1],), dtype=np.float32) for s in self.Ws]
        self.W_out = glorot_init(sizes[-1], output_size)
        self.b_out = np.zeros((output_size,), dtype=np.float32)

        # Novelty memory
        self.novelty_mem = deque(maxlen=novelty_mem)
        self.novelty_boost = novelty_boost

    def encode(self, x: np.ndarray) -> np.ndarray:
        # x shape (batch, input_size)
        z = x.dot(self.W_enc) + self.b_enc
        return tanh(z)

    def infer(self, pixel_vec: np.ndarray, S_vec: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        # pixel_vec (B, pixel_size), S_vec (B, pixel_size) -> concat -> forward
        inp = np.concatenate([pixel_vec, S_vec], axis=1)
        h = inp
        for W, b in zip(self.Ws, self.bs):
            h = relu(h.dot(W) + b)
        out = h.dot(self.W_out) + self.b_out
        return out, h

    def novelty_score(self, pixel_vec: np.ndarray) -> np.ndarray:
        if len(self.novelty_mem) == 0:
            return np.ones((pixel_vec.shape[0],), dtype=np.float32)
        mem = np.stack(self.novelty_mem, axis=0)
        a2 = np.sum(pixel_vec**2, axis=1, keepdims=True)
        b2 = np.sum(mem**2, axis=1, keepdims=True).T
        ab = pixel_vec.dot(mem.T)
        d2 = a2 + b2 - 2*ab
        min_d2 = np.min(d2, axis=1)
        min_d2 = np.maximum(min_d2, 0.0) + 1e-12
        scores = np.sqrt(min_d2) / (np.sqrt(np.mean(min_d2)) + 1e-12)
        return scores.astype(np.float32)

    def _update_latent_state(self, pixel_vec: np.ndarray, nov_scores: np.ndarray):
        # pixel_vec (B, pixel_size). Update self.S sequentially for each sample in the batch.
        for i in range(pixel_vec.shape[0]):
            p = pixel_vec[i]
            concat = np.concatenate([p, self.S])  # (2*pixel_size,)
            z = concat.dot(self.W_write) + self.b_write
            candidate = relu(z)
            g = sigmoid(concat.dot(self.W_gate) + self.b_gate)  # (pixel_size,)
            # novelty scaling: stronger writes for novel samples
            scale = 1.0 + (self.novelty_boost - 1.0) * (nov_scores[i] > 1.2).astype(np.float32)
            g = g * scale
            # clip gate
            g = np.clip(g, 0.0, 1.0)
            self.S = (1.0 - g) * self.S + g * candidate

    def step(self, x: np.ndarray, y: np.ndarray, epochs=1, batch_size=32, verbose=False):
        n = x.shape[0]
        indices = np.arange(n)
        for ep in range(epochs):
            np.random.shuffle(indices)
            for i in range(0, n, batch_size):
                batch_idx = indices[i:i+batch_size]
                xb = x[batch_idx]
                yb = y[batch_idx]

                pixel = self.encode(xb)            # (B, pixel_size)
                # expand current latent S to the batch
                S_batch = np.tile(self.S, (pixel.shape[0], 1))  # (B, pixel_size)

                preds, hidden = self.infer(pixel, S_batch)
                loss = mse(preds, yb)

                nov_scores = self.novelty_score(pixel)

                # compute gradients (vectorized across batch)
                grad_out = mse_grad(preds, yb)  # (B, out)
                grad_W_out = hidden.T.dot(grad_out)
                grad_b_out = np.sum(grad_out, axis=0)
                # backprop into last hidden
                grad_hidden = grad_out.dot(self.W_out.T) * relu_deriv(hidden)

                # build forward activations for backprop
                inp = np.concatenate([pixel, S_batch], axis=1)
                hs = [inp]
                zs = []
                h_temp = inp
                for W, b in zip(self.Ws, self.bs):
                    z_temp = h_temp.dot(W) + b
                    zs.append(z_temp)
                    h_temp = relu(z_temp)
                    hs.append(h_temp)

                g = grad_hidden
                grad_Ws = []
                grad_bs = []
                for layer_idx in range(len(self.Ws)-1, -1, -1):
                    h_prev = hs[layer_idx]
                    W = self.Ws[layer_idx]
                    z_curr = zs[layer_idx]
                    dZ = g * relu_deriv(z_curr)
                    grad_W = h_prev.T.dot(dZ)
                    grad_b = np.sum(dZ, axis=0)
                    grad_Ws.insert(0, grad_W)
                    grad_bs.insert(0, grad_b)
                    g = dZ.dot(W.T)

                # gradients w.r.t encoder parameters via pixel part only
                # g now is dL/d inp (B, 2*pixel_size); split into pixel and S parts
                grad_inp = g
                grad_pixel_part = grad_inp[:, :self.pixel_size]
                # tanh derivative for encoder output
                tanh_deriv = (1.0 - pixel**2)
                grad_W_enc = xb.T.dot(grad_pixel_part * tanh_deriv)
                grad_b_enc = np.sum(grad_pixel_part * tanh_deriv, axis=0)

                # optional decay
                if self.decay:
                    self.W_enc *= (1.0 - self.decay)
                    for k in range(len(self.Ws)):
                        self.Ws[k] *= (1.0 - self.decay)
                    self.W_out *= (1.0 - self.decay)
                    self.W_write *= (1.0 - self.decay)
                    self.W_gate *= (1.0 - self.decay)

                # apply gradient descent
                self.W_out -= self.lr * grad_W_out
                self.b_out -= self.lr * grad_b_out
                for k in range(len(self.Ws)):
                    self.Ws[k] -= self.lr * grad_Ws[k]
                    self.bs[k] -= self.lr * grad_bs[k]
                self.W_enc -= self.lr * grad_W_enc
                self.b_enc -= self.lr * grad_b_enc

                # update latent state (no gradients through this; stateful external update)
                self._update_latent_state(pixel, nov_scores)

                # update novelty memory
                for p in pixel:
                    self.novelty_mem.append(p.copy())

            if verbose:
                print(f"Epoch {ep+1}/{epochs} loss={loss:.6f}")

    def predict(self, x: np.ndarray) -> np.ndarray:
        pixel = self.encode(x)
        S_batch = np.tile(self.S, (pixel.shape[0], 1))
        out, _ = self.infer(pixel, S_batch)
        return out

    def save(self, path):
        # Save weights; Ws and bs saved as object arrays to preserve shapes
        np.savez(path,
                 W_enc=self.W_enc, b_enc=self.b_enc,
                 W_write=self.W_write, b_write=self.b_write,
                 W_gate=self.W_gate, b_gate=self.b_gate,
                 Ws=np.array(self.Ws, dtype=object), bs=np.array(self.bs, dtype=object),
                 W_out=self.W_out, b_out=self.b_out,
                 S=self.S)


# -----------------------------
# Benchmark task generators
# -----------------------------
def gen_boolean_gates():
    X_int = np.array([[a, b] for a in [0, 1] for b in [0, 1]], dtype=np.int32)
    X = X_int.astype(np.float32)
    Y_and = (X_int[:, 0] & X_int[:, 1]).astype(np.float32).reshape(-1, 1)
    Y_or  = (X_int[:, 0] | X_int[:, 1]).astype(np.float32).reshape(-1, 1)
    Y_xor = (X_int[:, 0] ^ X_int[:, 1]).astype(np.float32).reshape(-1, 1)
    return X, {'AND': Y_and, 'OR': Y_or, 'XOR': Y_xor}


def gen_parity(n_bits=8, samples=256):
    X = np.random.randint(0,2,size=(samples, n_bits)).astype(np.float32)
    Y = (np.sum(X, axis=1) % 2).reshape(-1,1).astype(np.float32)
    return X, Y


def gen_arithmetic(func='sin', samples=256):
    xs = np.linspace(-2*math.pi, 2*math.pi, samples).astype(np.float32).reshape(-1,1)
    if func == 'sin':
        ys = np.sin(xs).astype(np.float32)
    elif func == 'square':
        ys = (xs**2).astype(np.float32)
    elif func == 'exp':
        ys = np.exp(xs/4).astype(np.float32)
    else:
        ys = xs
    return xs, ys


def gen_modular(mod=7, samples=256):
    xs = np.random.randint(0,100,size=(samples,1)).astype(np.float32)
    ys = (xs % mod).astype(np.float32) / float(mod)
    return xs, ys


def gen_sequence_next(symbols=10, length=6, samples=256):
    X = np.random.randint(0, symbols, size=(samples, length))
    Y = (np.roll(X, -1, axis=1))[:, -1]
    X_oh = np.zeros((samples, length*symbols), dtype=np.float32)
    for i in range(samples):
        for t in range(length):
            X_oh[i, t*symbols + X[i,t]] = 1.0
    Y_oh = np.zeros((samples, symbols), dtype=np.float32)
    for i in range(samples):
        Y_oh[i, Y[i]] = 1.0
    return X_oh, Y_oh


def gen_fibonacci(samples=256, length=6):
    def fib_seq(start0, start1, L):
        seq = [start0, start1]
        for _ in range(L-2):
            seq.append(seq[-1] + seq[-2])
        return seq
    X = np.zeros((samples, length), dtype=np.float32)
    Y = np.zeros((samples,1), dtype=np.float32)
    for i in range(samples):
        s0 = np.random.randint(0,3)
        s1 = np.random.randint(0,3)
        seq = fib_seq(s0,s1,length+1)
        X[i] = np.array(seq[:-1])
        Y[i] = seq[-1]
    X /= (np.max(X)+1e-12)
    Y /= (np.max(Y)+1e-12)
    return X, Y


def gen_edge_images(nx=8, samples=256):
    X = np.zeros((samples, nx*nx), dtype=np.float32)
    Y = np.zeros((samples,1), dtype=np.float32)
    for i in range(samples):
        t = np.random.choice(['vertical','horizontal','noise','blank'])
        img = np.zeros((nx,nx), dtype=np.float32)
        if t == 'vertical':
            col = np.random.randint(0,nx)
            img[:, :col] = 1.0
            Y[i] = 1.0
        elif t == 'horizontal':
            row = np.random.randint(0,nx)
            img[:row, :] = 1.0
            Y[i] = 1.0
        elif t == 'noise':
            img = np.random.rand(nx,nx)
            Y[i] = 0.0
        else:
            img = np.zeros((nx,nx))
            Y[i] = 0.0
        X[i] = img.reshape(-1)
    return X, Y


def gen_gaussian_blobs(nx=8, samples=256):
    X = np.zeros((samples, nx*nx), dtype=np.float32)
    Y = np.zeros((samples,1), dtype=np.float32)
    for i in range(samples):
        img = np.zeros((nx,nx), dtype=np.float32)
        cx = np.random.uniform(0, nx)
        cy = np.random.uniform(0, nx)
        sigma = np.random.uniform(0.8, 1.6)
        for r in range(nx):
            for c in range(nx):
                d2 = (r-cy)**2 + (c-cx)**2
                img[r,c] = math.exp(-d2/(2*sigma*sigma))
        center_dist = math.hypot(cx - nx/2, cy - nx/2)
        Y[i] = 1.0 if center_dist < nx/4 else 0.0
        X[i] = img.reshape(-1)
    return X, Y


def gen_waveform_predict(samples=256, history=16):
    X = np.zeros((samples, history), dtype=np.float32)
    Y = np.zeros((samples,1), dtype=np.float32)
    for i in range(samples):
        freq = np.random.uniform(0.5, 3.0)
        phase = np.random.uniform(0, 2*math.pi)
        t = np.linspace(0, 1, history+1)
        wave = np.sin(2*math.pi*freq*t + phase) + 0.2*np.sign(np.sin(2*math.pi*freq*t + phase))
        X[i] = wave[:-1]
        Y[i] = wave[-1]
    return X, Y


def gen_delayed_xor(samples=256, delay=4):
    seq_len = delay + 2
    X_int = np.random.randint(0,2,size=(samples, seq_len)).astype(np.int32)
    Y = (X_int[:, -1] ^ X_int[:, -1-delay]).reshape(-1,1).astype(np.float32)
    Xf = X_int.reshape(samples, -1).astype(np.float32)
    return Xf, Y


def gen_autoencode_patterns(samples=256, size=16):
    X = (np.random.rand(samples, size) > 0.7).astype(np.float32)
    return X, X


def gen_novelty_test(samples=256, dim=32):
    center = np.zeros((dim,))
    X = np.random.randn(samples, dim) * 0.1
    for i in range(samples//10):
        X[i] += np.random.randn(dim) * 3.0
    Y = np.zeros((samples,1), dtype=np.float32)
    Y[:samples//10] = 1.0
    return X.astype(np.float32), Y


# -----------------------------
# Utilities: scaling helpers
# -----------------------------
def scale_pixel_shape(base_pixel_dim: int, scale: float) -> Tuple[int,int]:
    """
    Returns an integer pixel_shape (w,h) scaled from base.
    Rounds to nearest int and ensures at least 1.
    """
    s = max(1, int(round(base_pixel_dim * scale)))
    return (s, s)

def scale_hidden_sizes(base_sizes: Tuple[int,...], scale: float) -> Tuple[int,...]:
    return tuple(max(1, int(round(int(x) * scale))) for x in base_sizes)

def get_scaled_configs(args):
    """
    Returns a small config object/dict with scaled pixel_shape and common hidden size tuples.
    """
    scale = args.scale if hasattr(args, 'scale') and args.scale is not None else SCALE_FACTOR
    pixel_shape = scale_pixel_shape(args.pixel_dim if getattr(args, 'pixel_dim', None) else BASE_PIXEL_DIM, scale)
    hidden_small = scale_hidden_sizes(BASE_HIDDEN_SMALL, scale)
    hidden_med = scale_hidden_sizes(BASE_HIDDEN_MED, scale)
    hidden_large = scale_hidden_sizes(BASE_HIDDEN_LARGE, scale)
    return {
        'scale': scale,
        'pixel_shape': pixel_shape,
        'hidden_small': hidden_small,
        'hidden_med': hidden_med,
        'hidden_large': hidden_large
    }


# -----------------------------
# Benchmark runner utilities
# -----------------------------
def evaluate_regression(model, X, Y, epochs=50, batch=32, verbose=False):
    model.step(X, Y, epochs=epochs, batch_size=batch, verbose=verbose)
    preds = model.predict(X)
    loss = mse(preds, Y)
    return loss, preds


def accuracy_classification(preds, Y):
    if preds.shape[1] == 1:
        p = (preds[:,0] > 0.5).astype(np.float32)
        y = Y[:,0]
        return float(np.mean(p == y))
    else:
        p = np.argmax(preds, axis=1)
        y = np.argmax(Y, axis=1)
        return float(np.mean(p == y))


def save_results_csv(path, rows, header=None):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'w', newline='') as f:
        writer = csv.writer(f)
        if header:
            writer.writerow(header)
        for r in rows:
            writer.writerow(r)


# -----------------------------
# Individual benchmark implementations (now use scaled sizes)
# -----------------------------
def bench_low_level(args, out_dir, cfg):
    rows = []
    X, Ys = gen_boolean_gates()
    for name, Y in Ys.items():
        model = LiquidDataMatrix(input_size=X.shape[1],
                                 pixel_shape=cfg['pixel_shape'],
                                 hidden_sizes=cfg['hidden_small'],
                                 output_size=Y.shape[1],
                                 lr=args.lr,
                                 seed=1)
        loss, preds = evaluate_regression(model, X, Y, epochs=args.epochs, batch=8)
        acc = accuracy_classification(preds, Y)
        rows.append(["Boolean", name, loss, acc])

    Xp, Yp = gen_parity(n_bits=8, samples=128)
    model = LiquidDataMatrix(input_size=Xp.shape[1], pixel_shape=cfg['pixel_shape'],
                             hidden_sizes=cfg['hidden_med'], output_size=1, lr=args.lr, seed=2)
    loss, preds = evaluate_regression(model, Xp, Yp, epochs=args.epochs, batch=16)
    acc = accuracy_classification(preds, Yp)
    rows.append(["Parity", "8-bit", loss, acc])

    save_results_csv(os.path.join(out_dir, 'bench_low_level.csv'), rows, header=["category","task","loss","accuracy"])
    return rows


def bench_numerical(args, out_dir, cfg):
    rows = []
    for func in ['sin','square','exp']:
        X, Y = gen_arithmetic(func=func, samples=256)
        model = LiquidDataMatrix(input_size=X.shape[1], pixel_shape=cfg['pixel_shape'],
                                 hidden_sizes=cfg['hidden_med'], output_size=Y.shape[1], lr=args.lr, seed=3)
        loss, preds = evaluate_regression(model, X, Y, epochs=args.epochs, batch=32)
        rows.append(["Numerical", func, loss])
    X, Y = gen_modular(mod=7, samples=256)
    model = LiquidDataMatrix(input_size=X.shape[1], pixel_shape=cfg['pixel_shape'],
                             hidden_sizes=cfg['hidden_small'], output_size=1, lr=args.lr, seed=4)
    loss, preds = evaluate_regression(model, X, Y, epochs=args.epochs, batch=32)
    rows.append(["Numerical","mod7", loss])
    save_results_csv(os.path.join(out_dir, 'bench_numerical.csv'), rows, header=["category","task","loss"])
    return rows


def bench_sequence(args, out_dir, cfg):
    rows = []
    X, Y = gen_sequence_next(symbols=8, length=6, samples=256)
    model = LiquidDataMatrix(input_size=X.shape[1], pixel_shape=cfg['pixel_shape'],
                             hidden_sizes=cfg['hidden_large'], output_size=Y.shape[1], lr=args.lr, seed=5)
    loss, preds = evaluate_regression(model, X, Y, epochs=args.epochs, batch=32)
    acc = accuracy_classification(preds, Y)
    rows.append(["Sequence", "next-token", loss, acc])

    Xf, Yf = gen_fibonacci(samples=256, length=6)
    model = LiquidDataMatrix(input_size=Xf.shape[1], pixel_shape=cfg['pixel_shape'],
                             hidden_sizes=cfg['hidden_med'], output_size=Yf.shape[1], lr=args.lr, seed=6)
    loss, preds = evaluate_regression(model, Xf, Yf, epochs=args.epochs, batch=32)
    rows.append(["Sequence","Fibonacci","loss", loss])
    save_results_csv(os.path.join(out_dir, 'bench_sequence.csv'), rows, header=["category","task","loss","accuracy"])
    return rows


def bench_spatial(args, out_dir, cfg):
    rows = []
    nx = cfg['pixel_shape'][0]
    X, Y = gen_edge_images(nx=nx, samples=256)
    model = LiquidDataMatrix(input_size=X.shape[1], pixel_shape=cfg['pixel_shape'],
                             hidden_sizes=cfg['hidden_med'], output_size=1, lr=args.lr, seed=7)
    loss, preds = evaluate_regression(model, X, Y, epochs=args.epochs, batch=32)
    acc = accuracy_classification(preds, Y)
    rows.append(["Spatial","edges", loss, acc])

    Xg, Yg = gen_gaussian_blobs(nx=nx, samples=256)
    model = LiquidDataMatrix(input_size=Xg.shape[1], pixel_shape=cfg['pixel_shape'],
                             hidden_sizes=cfg['hidden_med'], output_size=1, lr=args.lr, seed=8)
    loss, preds = evaluate_regression(model, Xg, Yg, epochs=args.epochs, batch=32)
    acc = accuracy_classification(preds, Yg)
    rows.append(["Spatial","gauss_blobs", loss, acc])

    save_results_csv(os.path.join(out_dir, 'bench_spatial.csv'), rows, header=["category","task","loss","accuracy"])
    return rows


def bench_temporal(args, out_dir, cfg):
    rows = []
    X, Y = gen_waveform_predict(samples=256, history=16)
    model = LiquidDataMatrix(input_size=X.shape[1], pixel_shape=cfg['pixel_shape'],
                             hidden_sizes=cfg['hidden_med'], output_size=1, lr=args.lr, seed=9)
    loss, preds = evaluate_regression(model, X, Y, epochs=args.epochs, batch=32)
    rows.append(["Temporal","waveform", loss])

    Xd, Yd = gen_delayed_xor(samples=256, delay=4)
    model = LiquidDataMatrix(input_size=Xd.shape[1], pixel_shape=cfg['pixel_shape'],
                             hidden_sizes=cfg['hidden_med'], output_size=1, lr=args.lr, seed=10)
    loss, preds = evaluate_regression(model, Xd, Yd, epochs=args.epochs, batch=32)
    acc = accuracy_classification(preds, Yd)
    rows.append(["Temporal","delayed_xor", loss, acc])

    save_results_csv(os.path.join(out_dir, 'bench_temporal.csv'), rows, header=["category","task","loss","accuracy"])
    return rows


def bench_compression(args, out_dir, cfg):
    rows = []
    X, Y = gen_autoencode_patterns(samples=256, size=16)
    model = LiquidDataMatrix(input_size=X.shape[1], pixel_shape=cfg['pixel_shape'],
                             hidden_sizes=cfg['hidden_med'], output_size=Y.shape[1], lr=args.lr, seed=11)
    loss, preds = evaluate_regression(model, X, Y, epochs=args.epochs, batch=32)
    rows.append(["Compression","autoencode", loss])
    save_results_csv(os.path.join(out_dir, 'bench_compression.csv'), rows, header=["category","task","loss"])
    return rows


def bench_novelty(args, out_dir, cfg):
    rows = []
    X, Y = gen_novelty_test(samples=256, dim=32)
    model = LiquidDataMatrix(input_size=X.shape[1], pixel_shape=cfg['pixel_shape'],
                             hidden_sizes=cfg['hidden_small'], output_size=1, lr=args.lr, seed=12)
    model.step(X, Y, epochs=args.epochs, batch_size=32)
    pixel = model.encode(X)
    scores = model.novelty_score(pixel)
    novel_mean = np.mean(scores[Y[:,0] == 1]) if np.any(Y[:,0] == 1) else 0.0
    normal_mean = np.mean(scores[Y[:,0] == 0]) if np.any(Y[:,0] == 0) else 0.0
    rows.append(["Novelty","mean_novel_score", float(novel_mean)])
    rows.append(["Novelty","mean_normal_score", float(normal_mean)])
    save_results_csv(os.path.join(out_dir, 'bench_novelty.csv'), rows, header=["category","task","value"])
    return rows


def bench_efficiency(args, out_dir, cfg):
    rows = []
    for samples in [64, 256, 1024]:
        X = np.random.randn(samples, 32).astype(np.float32)
        Y = np.random.randn(samples, 1).astype(np.float32)
        t0 = time.time()
        model = LiquidDataMatrix(input_size=X.shape[1], pixel_shape=cfg['pixel_shape'],
                                 hidden_sizes=cfg['hidden_med'], output_size=1, lr=args.lr, seed=20)
        model.step(X, Y, epochs=3, batch_size=32)
        dt = time.time() - t0
        rows.append(["efficiency","samples", samples, "time_s", dt])

    X = np.random.randn(256, 32).astype(np.float32)
    Y = np.random.randn(256,1).astype(np.float32)
    model = LiquidDataMatrix(input_size=X.shape[1], pixel_shape=cfg['pixel_shape'],
                             hidden_sizes=cfg['hidden_med'], output_size=1, lr=args.lr, seed=21, decay=0.01)
    model.step(X, Y, epochs=10, batch_size=32)
    W_before = model.W_out.copy()
    model.step(X, Y, epochs=5, batch_size=32)
    W_after = model.W_out
    w_change = np.linalg.norm(W_after - W_before)
    rows.append(["efficiency","weight_change_with_decay", float(w_change)])

    save_results_csv(os.path.join(out_dir, 'bench_efficiency.csv'), rows, header=["category","metric","val1","label","val2"])
    return rows


# -----------------------------
# High-level runner
# -----------------------------
def run_all(args):
    out_dir = args.output_dir
    try:
        os.makedirs(out_dir, exist_ok=True)
    except PermissionError:
        home_default = os.path.join(os.path.expanduser('~'), 'ldm_results')
        try:
            os.makedirs(home_default, exist_ok=True)
            print(f"Warning: cannot create '{out_dir}'. Using home folder: {home_default}")
            out_dir = home_default
        except PermissionError:
            fallback = os.path.join(tempfile.gettempdir(), 'ldm_results')
            os.makedirs(fallback, exist_ok=True)
            print(f"Warning: cannot create '{out_dir}' or '{home_default}'. Falling back to temp: {fallback}")
            out_dir = fallback

    cfg = get_scaled_configs(args)
    print(f"Running with scale={cfg['scale']} pixel_shape={cfg['pixel_shape']} hidden_small={cfg['hidden_small']} hidden_med={cfg['hidden_med']} hidden_large={cfg['hidden_large']}")

    all_rows = {}
    if args.benchmarks is None or 1 in args.benchmarks:
        all_rows['low_level'] = bench_low_level(args, out_dir, cfg)
    if args.benchmarks is None or 2 in args.benchmarks:
        all_rows['numerical'] = bench_numerical(args, out_dir, cfg)
    if args.benchmarks is None or 3 in args.benchmarks:
        all_rows['sequence'] = bench_sequence(args, out_dir, cfg)
    if args.benchmarks is None or 4 in args.benchmarks:
        all_rows['spatial'] = bench_spatial(args, out_dir, cfg)
    if args.benchmarks is None or 5 in args.benchmarks:
        all_rows['temporal'] = bench_temporal(args, out_dir, cfg)
    if args.benchmarks is None or 6 in args.benchmarks:
        all_rows['compression'] = bench_compression(args, out_dir, cfg)
    if args.benchmarks is None or 7 in args.benchmarks:
        all_rows['novelty'] = bench_novelty(args, out_dir, cfg)
    if args.benchmarks is None or 8 in args.benchmarks:
        all_rows['efficiency'] = bench_efficiency(args, out_dir, cfg)

    # Primary summary saved to chosen out_dir
    summary_path = os.path.join(out_dir, 'summary.txt')
    with open(summary_path, 'w') as f:
        for k, rows in all_rows.items():
            f.write("== " + str(k) + " ==\n")
            for r in rows:
                f.write(','.join(map(str, r)) + '\n')
            f.write('\n')

    # Also attempt to place a copy in the user's Downloads folder
    downloads_dir = os.path.join(os.path.expanduser('~'), 'Downloads')
    downloads_summary_path = os.path.join(downloads_dir, DOWNLOADS_COPY_NAME)
    try:
        os.makedirs(downloads_dir, exist_ok=True)
        with open(downloads_summary_path, 'w') as f:
            for k, rows in all_rows.items():
                f.write("== " + str(k) + " ==\n")
                for r in rows:
                    f.write(','.join(map(str, r)) + '\n')
                f.write('\n')
        print(f"Also wrote summary to your Downloads folder: {downloads_summary_path}")
    except PermissionError:
        print(f"Warning: couldn't write to Downloads folder ({downloads_dir}). Summary remains in: {summary_path}")
    except Exception:
        # Generic fallback: copy the primary summary into the system temp dir
        fallback_summary = os.path.join(tempfile.gettempdir(), DOWNLOADS_COPY_NAME)
        try:
            with open(fallback_summary, 'w') as f:
                for k, rows in all_rows.items():
                    f.write("== " + str(k) + " ==\n")
                    for r in rows:
                        f.write(','.join(map(str, r)) + '\n')
                    f.write('\n')
            print(f"Could not write to Downloads; wrote summary to temporary file: {fallback_summary}")
        except Exception:
            print(f"Could not write summary to Downloads or temporary folder. Summary remains in: {summary_path}")

    # Optionally show a GUI with the summary
    if not args.no_gui:
        try:
            show_summary_gui(summary_path, downloads_summary_path)
        except Exception as e:
            print(f"GUI not available or failed: {e}")

    return out_dir


# -----------------------------
# Simple Tkinter GUI to show the summary
# -----------------------------
def show_summary_gui(summary_path, downloads_copy_path):
    """
    Opens a simple Tkinter window that displays the summary text and has:
    - Refresh button
    - Save as... button
    - Path labels
    If Tkinter is not available, raises ImportError.
    """
    try:
        import tkinter as tk
        from tkinter import filedialog, scrolledtext, messagebox
    except Exception as e:
        raise ImportError("tkinter not available") from e

    root = tk.Tk()
    root.title("LDM Summary Viewer")

    frm_paths = tk.Frame(root)
    frm_paths.pack(fill='x', padx=6, pady=4)

    lbl_primary = tk.Label(frm_paths, text=f"Primary summary: {summary_path}")
    lbl_primary.pack(anchor='w')
    lbl_download = tk.Label(frm_paths, text=f"Downloads copy: {downloads_copy_path}")
    lbl_download.pack(anchor='w')

    txt = scrolledtext.ScrolledText(root, width=100, height=30)
    txt.pack(fill='both', expand=True, padx=6, pady=6)

    def load_summary():
        txt.delete('1.0', tk.END)
        p = summary_path if os.path.exists(summary_path) else (downloads_copy_path if os.path.exists(downloads_copy_path) else None)
        if p is None:
            txt.insert(tk.END, "No summary file found.")
            return
        try:
            with open(p, 'r') as f:
                txt.insert(tk.END, f.read())
        except Exception as e:
            txt.insert(tk.END, f"Failed to read {p}: {e}")

    def save_as():
        p = filedialog.asksaveasfilename(defaultextension=".txt", filetypes=[("Text file","*.txt"),("All files","*.*")])
        if not p:
            return
        try:
            with open(p, 'w') as f:
                f.write(txt.get('1.0', tk.END))
            messagebox.showinfo("Saved", f"Saved summary to {p}")
        except Exception as e:
            messagebox.showerror("Error", f"Failed to save: {e}")

    btns = tk.Frame(root)
    btns.pack(fill='x', padx=6, pady=6)
    b_refresh = tk.Button(btns, text="Refresh", command=load_summary)
    b_refresh.pack(side='left')
    b_save = tk.Button(btns, text="Save as...", command=save_as)
    b_save.pack(side='left', padx=(6,0))
    b_close = tk.Button(btns, text="Close", command=root.destroy)
    b_close.pack(side='right')

    # load immediately
    load_summary()
    # run GUI mainloop without blocking if running in non-interactive console? We will block (that's expected).
    root.mainloop()


# -----------------------------
# CLI
# -----------------------------
def parse_args():
    p = argparse.ArgumentParser(description='LDM Benchmark Suite (scaled + GUI summary)')
    p.add_argument('--all', action='store_true', help='Run all benchmarks (alias)')
    p.add_argument('--bench', dest='benchmarks', nargs='+', type=int,
                   help='Which benchmarks to run (1..8)')
    p.add_argument('--epochs', type=int, default=DEFAULT_EPOCHS, help='Epochs per task (small default for mobile)')
    p.add_argument('--lr', type=float, default=DEFAULT_LR, help='Learning rate')
    p.add_argument('--output-dir', dest='output_dir', default=os.path.join(os.path.expanduser('~'), 'ldm_results'), help='Output directory')
    p.add_argument('--no-plot', action='store_true', dest='no_plot', help='Disable plotting')
    p.add_argument('--no-gui', action='store_true', dest='no_gui', help='Disable GUI summary viewer')
    p.add_argument('--scale', type=float, default=None, help='Global scale factor to multiply sizes (overrides config SCALE_FACTOR)')
    p.add_argument('--pixel-dim', dest='pixel_dim', type=int, default=None, help='Base pixel dimension override (overrides BASE_PIXEL_DIM)')
    args = p.parse_args()
    if args.all:
        args.benchmarks = None
    return args


def main():
    args = parse_args()
    if args.no_plot:
        global _HAS_MPL
        _HAS_MPL = False

    # if no explicit scale passed, pass None and get_scaled_configs will use the CONFIG SCALE_FACTOR
    if args.scale is None:
        args.scale = SCALE_FACTOR

    out_dir = run_all(args)
    print(f"Benchmarks finished. Results in: {out_dir}")
    print("CSV files per benchmark and summary.txt created.")


if __name__ == '__main__':
    main()
